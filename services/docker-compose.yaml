services:
  master:
    image: my-spark-py312:20253011_0900
    container_name: master
    environment:
      - SPARK_MODE=master
      - SPARK_NO_DAEMONIZE=1
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=file:/tmp/spark-events
    ports:
      - "8880:8080"
      - "7077:7077"
    command: ["/opt/spark/sbin/start-master.sh"]
    networks: [spark_network]
    volumes:
      - ./spark/data:/data:ro
      - spark-events:/tmp/spark-events
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

  worker_1:
    image: my-spark-py312:20253011_0900
    container_name: worker_1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_NO_DAEMONIZE=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=file:/tmp/spark-events
    depends_on: [master]
    command: ["/opt/spark/sbin/start-worker.sh","spark://master:7077"]
    ports:
      - "8881:8881"
    networks: [spark_network]
    volumes:
      - ./spark/data:/data:ro
      - spark-events:/tmp/spark-events
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  worker_2:
    image: my-spark-py312:20253011_0900
    container_name: worker_2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://master:7077
      - SPARK_NO_DAEMONIZE=1
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=file:/tmp/spark-events
    depends_on: [master]
    command: ["/opt/spark/sbin/start-worker.sh","spark://master:7077"]
    ports:
      - "8882:8882"
    networks: [spark_network]
    volumes:
      - ./spark/data:/data:ro
      - spark-events:/tmp/spark-events
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
  
  history:
    image: my-spark-py312:20253011_0900
    container_name: spark-history
    environment:
      - SPARK_NO_DAEMONIZE=1
    command: ["/opt/spark/sbin/start-history-server.sh"]
    ports:
      - "18080:18080"
    volumes:
      - spark-events:/tmp/spark-events
    networks: [spark_network]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:18080"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  jupyter:
    image: my-jupyter-py312:20253011_1200
    container_name: jupyter
    ports:
      - "8888:8888"
    volumes:
      - ./jupyter/notebooks:/work
      - spark-events:/tmp/spark-events
    working_dir: /work
    command: >
      jupyter lab
      --ip=0.0.0.0
      --port=8888
      --no-browser
      --IdentityProvider.token=dev
      --allow-root
    networks: [spark_network]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/lab"]
      interval: 20s
      timeout: 10s
      retries: 5
      start_period: 30s

  postgres:
    image: my-postgres:20252911_0900
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
      TZ: Asia/Bangkok
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks: [spark_network]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 30s
    restart: unless-stopped

  redis:
    image: my-redis:20252911_0900
    container_name: redis
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - redis_data:/data
    networks: [spark_network]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 10
    restart: unless-stopped

  airflow-init:
    # build:
    #   context: .
    #   dockerfile: airflow/Dockerfile 
    image: my-airflow-py312:20252911_1800
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-41frjvXyB6uoLBPmYuKXwwXstY9aQTxUrB0Yv0cBdOQ=}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW__WEBSERVER__AUTHENTICATE: "True"
      AIRFLOW__WEBSERVER__RBAC: "True"
      AIRFLOW__WEBSERVER__RBAC_AUTH_USER_REGISTRATION: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY:-41frjvXyB6uoLBPmYuKXwwXstY9aQTxUrB0Yv0cBdOQ=}
      TZ: Asia/Bangkok
      AIRFLOW_ADMIN_USER: ${AIRFLOW_ADMIN_USER:-airflow}
      AIRFLOW_ADMIN_PASS: ${AIRFLOW_ADMIN_PASS:-airflow123}
      AIRFLOW_ADMIN_EMAIL: ${AIRFLOW_ADMIN_EMAIL:-airflow@localhost.com}
    volumes:
      - ./airflow/dags:/opt/airflow/dags:rw
      - ./airflow/logs:/opt/airflow/logs:rw
      - ./airflow/plugins:/opt/airflow/plugins:rw
    networks: [spark_network]
    restart: "no"
    command: |
      bash -c "airflow db init && 
              airflow users create -u airflow -p airflow123 -f Admin -l User -r Admin -e airflow@localhost.com"
    
  airflow-webserver:
    image: my-airflow-py312:20252911_1800
    container_name: airflow-webserver
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-41frjvXyB6uoLBPmYuKXwwXstY9aQTxUrB0Yv0cBdOQ=}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW__WEBSERVER__AUTHENTICATE: "True"
      AIRFLOW__WEBSERVER__RBAC: "True"
      AIRFLOW__WEBSERVER__RBAC_AUTH_USER_REGISTRATION: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY:-41frjvXyB6uoLBPmYuKXwwXstY9aQTxUrB0Yv0cBdOQ=}
      TZ: Asia/Bangkok
    ports:
      - "8083:8080"
    command: bash -c "airflow webserver"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    volumes:
      - ./airflow/dags:/opt/airflow/dags:rw
      - ./airflow/logs:/opt/airflow/logs:rw
      - ./airflow/plugins:/opt/airflow/plugins:rw
    networks: [spark_network]
    restart: unless-stopped

  airflow-scheduler:
    image: my-airflow-py312:20252911_1800
    container_name: airflow-scheduler
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-41frjvXyB6uoLBPmYuKXwwXstY9aQTxUrB0Yv0cBdOQ=}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW__WEBSERVER__AUTHENTICATE: "True"
      AIRFLOW__WEBSERVER__RBAC: "True"
      AIRFLOW__WEBSERVER__RBAC_AUTH_USER_REGISTRATION: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY:-41frjvXyB6uoLBPmYuKXwwXstY9aQTxUrB0Yv0cBdOQ=}
      TZ: Asia/Bangkok
    command: bash -lc "airflow scheduler"
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    volumes:
      - ./airflow/dags:/opt/airflow/dags:rw
      - ./airflow/logs:/opt/airflow/logs:rw
      - ./airflow/plugins:/opt/airflow/plugins:rw
    networks: [spark_network]
    restart: unless-stopped

  airflow-worker:
    image: my-airflow-py312:20252911_1800
    container_name: airflow-worker
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-41frjvXyB6uoLBPmYuKXwwXstY9aQTxUrB0Yv0cBdOQ=}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW__WEBSERVER__AUTHENTICATE: "True"
      AIRFLOW__WEBSERVER__RBAC: "True"
      AIRFLOW__WEBSERVER__RBAC_AUTH_USER_REGISTRATION: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY:-41frjvXyB6uoLBPmYuKXwwXstY9aQTxUrB0Yv0cBdOQ=}
      TZ: Asia/Bangkok
    command: bash -lc "airflow celery worker"
    healthcheck:
      test: ["CMD-SHELL", "celery -A airflow.executors.celery_executor.app status | grep 'OK'"]
      interval: 30s
      timeout: 20s
      retries: 5
      start_period: 60s
    volumes:
      - ./airflow/dags:/opt/airflow/dags:rw
      - ./airflow/logs:/opt/airflow/logs:rw
      - ./airflow/plugins:/opt/airflow/plugins:rw
    networks: [spark_network]
    restart: unless-stopped
  

  airflow-trigger:
    image: my-airflow-py312:20252911_1800
    container_name: airflow-trigger
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-41frjvXyB6uoLBPmYuKXwwXstY9aQTxUrB0Yv0cBdOQ=}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW__WEBSERVER__AUTHENTICATE: "True"
      AIRFLOW__WEBSERVER__RBAC: "True"
      AIRFLOW__WEBSERVER__RBAC_AUTH_USER_REGISTRATION: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_SECRET_KEY:-41frjvXyB6uoLBPmYuKXwwXstY9aQTxUrB0Yv0cBdOQ=}
      TZ: Asia/Bangkok
    command: bash -lc "airflow triggerer"
    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep 'airflow triggerer' | grep -qv grep"]
      interval: 30s
      timeout: 20s
      retries: 5
      start_period: 60s
    volumes:
      - ./airflow/dags:/opt/airflow/dags:rw
      - ./airflow/logs:/opt/airflow/logs:rw
      - ./airflow/plugins:/opt/airflow/plugins:rw
    networks: [spark_network]
    restart: unless-stopped

  gitlab:
    # build:
    #   context: ./gitlab
    #   dockerfile: Dockerfile
    image: my-gitlab:20252911_0900
    container_name: gitlab
    hostname: gitlab
    shm_size: "1g"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9084/-/readiness || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 300s
    environment:
      EXTERNAL_URL: ${GITLAB_EXTERNAL_URL:-http://localhost:9084}
      TZ: Asia/Bangkok
      SSH_PORT: ${GITLAB_SSH_PORT:-2222}
      REGISTRY_ENABLE: "false"
      REGISTRY_EXTERNAL_URL: ${GITLAB_REGISTRY_EXTERNAL_URL:-}
      LETSENCRYPT_ENABLE: "false"
      GITLAB_ROOT_PASSWORD: "G!tLbS#cret2025"
    ports:
      - "9084:9084"
      - "8084:80"
      - "8443:443"
      - "2222:22"
    volumes:
      - gitlab_config:/etc/gitlab
      - gitlab_logs:/var/log/gitlab
      - gitlab_data:/var/opt/gitlab
    networks: [spark_network]
    restart: unless-stopped
    depends_on:
      - postgres
      - redis
  
  minio:
    image: my-minio:20252911_0900
    container_name: minio-cs
    command: server --address ":9990" --console-address ":9991" /data
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    volumes:
      - ./minio/data:/data
    ports:
      - "9990:9990"
      - "9991:9991"
      - "9992:9992"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9990/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    networks: [spark_network]

networks:
  spark_network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  gitlab_data:
  gitlab_config:
  gitlab_logs:
  spark-events:
