x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "50m"
    max-file: "20"  
    compress: "true"

x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE}
  environment:
    - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
    - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
    - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/airflow
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
    - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/airflow
    - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true
    - AIRFLOW__WEBSERVER__AUTHENTICATE=True
    - AIRFLOW__WEBSERVER__RBAC=True
    - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
    - TZ=${TZ}
  volumes:
    - ./airflow/dags:/opt/airflow/dags:rw
    - ./airflow/logs:/opt/airflow/logs:rw
    - ./airflow/plugins:/opt/airflow/plugins:rw
    - spark-events:/tmp/spark-events:rw
  networks: [spark_network]
  depends_on:
    postgres:
      condition: service_healthy
    redis:
      condition: service_healthy
  logging: *default-logging

services:
  # ==============
  # Spark Services
  # ==============
  
  master:
    image: ${SPARK_IMAGE}
    environment:
      - SPARK_MODE=master
      - SPARK_NO_DAEMONIZE=1
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=file:/tmp/spark-events
    ports:
      - "${SPARK_MASTER_WEBUI_PORT}:8080"
    command: ["/opt/spark/sbin/start-master.sh"]
    networks: [spark_network]
    volumes:
      - spark-events:/tmp/spark-events
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    deploy:
      resources:
        limits:
          cpus: '10.0'
          memory: 13G
        reservations:
          memory: 256M
    logging: *default-logging
    restart: always

  worker_1:
    image: ${SPARK_IMAGE}
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://master:7077
      - SPARK_NO_DAEMONIZE=1
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=file:/tmp/spark-events
    depends_on: [master]
    command: ["/opt/spark/sbin/start-worker.sh","spark://master:7077"]
    networks: [spark_network]
    volumes:
      - spark-events:/tmp/spark-events
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: '10.0'
          memory: 13G
        reservations:
          memory: 256M
    logging: *default-logging
    restart: always

  # worker_2:
  #   image: ${SPARK_IMAGE}
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://master:7077
  #     - SPARK_NO_DAEMONIZE=1
  #     - SPARK_EVENTLOG_ENABLED=true
  #     - SPARK_EVENTLOG_DIR=file:/tmp/spark-events
  #   depends_on: [master]
  #   command: ["/opt/spark/sbin/start-worker.sh","spark://master:7077"]
  #   networks: [spark_network]
  #   volumes:
  #     - spark-events:/tmp/spark-events
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8081"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 20s
  #  deploy:
  #       resources:
  #         limits:
  #           cpus: '10.0'
  #           memory: 13G
  #         reservations:
  #           memory: 256M
  #     logging: *default-logging
  #     restart: always
  
  history:
    image: ${SPARK_IMAGE}
    environment:
      - SPARK_NO_DAEMONIZE=1
    command: ["/opt/spark/sbin/start-history-server.sh"]
    ports:
      - "${SPARK_HISTORY_PORT}:18080"
    volumes:
      - spark-events:/tmp/spark-events
    networks: [spark_network]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:18080"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          memory: 256M
    logging: *default-logging
    restart: always

  # ==================
  # JupyterLab Service
  # ==================

  jupyter:
    image: ${JUPYTER_IMAGE}
    ports:
      - "${JUPYTER_PORT}:8888"
      - "${SPARK_UI_PORT_START}-${SPARK_UI_PORT_END}:${SPARK_UI_PORT_START}-${SPARK_UI_PORT_END}"
    volumes:
      - ./jupyter/notebooks:/work
      - spark-events:/tmp/spark-events
    working_dir: /work
    command: >
      jupyter lab --ip=0.0.0.0 --port=8888 --no-browser 
      --IdentityProvider.token='${JUPYTER_TOKEN}' --allow-root
    networks: [spark_network]
    environment:
      - TERM=xterm-256color
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/lab"]
      interval: 20s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '10.0'
          memory: 13G
        reservations:
          memory: 256M
    logging: *default-logging
    restart: always

  # ================
  # Airflow Services
  # ================

  airflow-init:
    <<: *airflow-common
    restart: "no"
    user: "0:0"
    command: >
      bash -c '
        mkdir -p /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins /tmp/spark-events &&
        chown -R 50000:0 /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins /tmp/spark-events &&
        airflow db init &&
        airflow users create -u ${AIRFLOW_ADMIN_USER} -p ${AIRFLOW_ADMIN_PASS} -f Admin -l User -r Admin -e ${AIRFLOW_ADMIN_EMAIL}
      '
    
  airflow-webserver:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    ports:
      - "${AIRFLOW_WEBSERVER_PORT}:8080"
    command: bash -c "airflow webserver"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          memory: 256M
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: bash -lc "airflow scheduler"
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          memory: 256M
    restart: always

  airflow-worker:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: bash -lc "airflow celery worker"
    healthcheck:
      test: ["CMD-SHELL", "celery -A airflow.executors.celery_executor.app status | grep 'OK'"]
      interval: 30s
      timeout: 20s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '10.0'
          memory: 13G
        reservations:
          memory: 256M
    restart: always
  
  airflow-trigger:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: bash -lc "airflow triggerer"
    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep 'airflow triggerer' | grep -qv grep"]
      interval: 30s
      timeout: 20s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 2G
        reservations:
          memory: 256M
    restart: always

  # ===============
  # Gitea Services
  # ===============

  gitea:
    image: ${GITEA_IMAGE}
    environment:
        - USER_UID=1000
        - USER_GID=1000
        - GITEA__database__DB_TYPE=postgres
        - GITEA__database__HOST=postgres:5432
        - GITEA__database__NAME=gitea
        - GITEA__database__USER=${POSTGRES_USER}
        - GITEA__database__PASSWD=${POSTGRES_PASSWORD}
        - GITEA__actions__ENABLED=true
        - GITEA__server__APP_DATA_PATH=/data/gitea
        - GITEA__server__DOMAIN=localhost
        - GITEA__server__SSH_DOMAIN=localhost
        - GITEA__server__HTTP_PORT=3000
        - GITEA__server__ROOT_URL=http://localhost:${GITEA_HTTP_PORT}/ 
        - GITEA__server__SSH_PORT=${GITEA_SSH_PORT}
        - GITEA__server__LFS_START_SERVER=true
        - GITEA__security__INSTALL_LOCK=true 
        - GITEA__service__DISABLE_REGISTRATION=false
        - GITEA__actions__ENABLED=true
    ports:
      - "${GITEA_HTTP_PORT}:3000"
      - "${GITEA_SSH_PORT}:22"
    volumes:
      - gitea_data:/data
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    networks: [spark_network]
    depends_on:
      postgres:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '4.0'
        reservations:
          memory: 256M
          cpus: '0.5'
    logging: *default-logging
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    
  gitea-init:
    image: ${GITEA_IMAGE}
    volumes:
      - gitea_data:/data
      - /etc/timezone:/etc/timezone:ro
    networks: [spark_network]
    depends_on:
      gitea:
        condition: service_healthy
      postgres:
        condition: service_healthy
    user: "1000:1000"
    environment:
      - ADMIN_USER=${GITEA_ADMIN_USER}
      - ADMIN_PASS=${GITEA_ADMIN_PASS}
      - ADMIN_EMAIL=${GITEA_ADMIN_EMAIL}
    command: >
      bash -c '
        echo "Waiting for Gitea Main Service..."
        while [ ! -f /data/gitea/conf/app.ini ]; do sleep 2; done
        sleep 5 
        
        echo "Creating Admin User ($$ADMIN_USER)..."
        /usr/local/bin/gitea admin user create --admin \
          --username "$$ADMIN_USER" \
          --password "$$ADMIN_PASS" \
          --email "$$ADMIN_EMAIL" \
          --config /data/gitea/conf/app.ini || echo "Admin user already exists or creation failed"
          
        echo "Done! Exiting..."
      '
    restart: "no"
    deploy:
      resources:
        limits:
          memory: 256M
    
    # gitea-runner:
    #   image: my-gitea-runner:20251217_0900
    #   depends_on:
    #     - gitea
    #   environment:
    #     - GITEA_INSTANCE_URL=http://gitea:3000
    #     - GITEA_RUNNER_REGISTRATION_TOKEN=${GITEA_RUNNER_TOKEN} 
    #     - GITEA_RUNNER_NAME=local-runner
    #     - GITEA_RUNNER_LABELS=ubuntu-latest:docker://node:16-bullseye,ubuntu-22.04:docker://node:16-bullseye
    #   volumes:
    #     - ./gitea/runner-data:/data
    #     - /var/run/docker.sock:/var/run/docker.sock
    #   networks: [spark_network]
    #   deploy:
    #     resources:
    #       limits:
    #         memory: 2G
    #         cpus: '4.0'
    #   logging: *default-logging 
    #   restart: always
    #   healthcheck:
    #     test: ["CMD-SHELL", "pgrep act_runner > /dev/null || exit 1"]
    #     interval: 30s
    #     timeout: 5s
    #     retries: 3
    #     start_period: 20s

  # =======================================================
  # Storage and Database Services: MinIO, PostgreSQL, Redis
  # =======================================================

  minio:
    image: ${MINIO_IMAGE}
    command: server --address ":9990" --console-address ":9991" /data
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - minio_data:/data
    ports:
      - "${MINIO_API_PORT}:9990"
      - "${MINIO_CONSOLE_PORT}:9991"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9990/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    networks: [spark_network]
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 2G
        reservations:
          memory: 128M
    logging: *default-logging
    restart: always
  
  postgres:
    image: ${POSTGRES_IMAGE}
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      TZ: Asia/Bangkok
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks: [spark_network]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 2G
    logging: *default-logging
    restart: always
  
  postgres_ui:
    image: ${POSTGRES_UI_IMAGE}
    environment:
      - DATABASE_URL=postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/postgres?sslmode=disable
    ports:
      - "${POSTGRES_UI_PORT}:8081"
    networks: [spark_network]
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
    logging: *default-logging
    restart: always

  redis:
    image: ${REDIS_IMAGE}
    hostname: redis
    command: ["redis-server", "--appendonly", "yes"]
    volumes:
      - redis_data:/data
    networks: [spark_network]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 10
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
    logging: *default-logging
    restart: always
  
  # redis-ui:
  #   image: my-redis-ui:20250112_0900 # or "ghcr.io/joeferner/redis-commander:latest"
  #   hostname: redis-ui
  #   restart: always
  #   environment:
  #   - REDIS_HOSTS=local:redis:6379
  #   ports:
  #   - "18081:8081"
  #   user: redis
  #   networks: [spark_network]
  #   healthcheck:
  #     test: ["CMD", "wget", "-qO-", "http://127.0.0.1:8081/"]
  #     interval: 10s
  #     timeout: 3s
  #     retries: 5
    #   start_period: 20s
    # deploy:
    #     resources:
    #       limits:
    #         cpus: '10.0'
    #         memory: 13G
    #   logging: *default-logging
    #   restart: always

networks:
  spark_network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  gitea_data:
  minio_data:
  spark-events:
